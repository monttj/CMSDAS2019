{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic ML exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load necessary packages first. Makesure you are using tensorflow kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "import ROOT\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU is available, we can use GPU with following line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read ntuple within RDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal = ROOT.RDataFrame(\"mytree\", \"myntuple_ttH.root\")\n",
    "df_background = ROOT.RDataFrame(\"mytree\", \"myntuple_ttbb.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change root data format directly to numpy within RDataFrame. Then will check the format of the numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_p = df_signal.AsNumpy()\n",
    "df_train_n = df_background.AsNumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Read-out of the full RDataFrame:\\n{}\\n\".format(df_train_p))\n",
    "print(\"Read-out of the full RDataFrame:\\n{}\\n\".format(df_train_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a keyword and corresponding values for each event as columns. So we will stack all variables that will be used as input for deep neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p = np.vstack( ( df_train_p[\"dR\"], df_train_p[\"mbb\"]  ))\n",
    "train_n = np.vstack( ( df_train_n[\"dR\"], df_train_n[\"mbb\"]  ))\n",
    "print( train_p.shape )\n",
    "print(train_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, will need to change the shape so that column represents variables and row represents each event. Compare the shape of the matrix. (Please let me know if you have a better idea to simply this procedure.)\n",
    "\n",
    "And check number of events for signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p = np.transpose(train_p)\n",
    "train_n = np.transpose(train_n)\n",
    "print( train_p.shape )\n",
    "print(train_p)\n",
    "print(\"signal events = \",  len(train_p))\n",
    "print(\"background events = \",  len(train_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will label the signal as 1 and background as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.vstack(( train_p, train_n))\n",
    "train_label = np.array([1]*len(train_p)+[0]*len(train_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbertr=len(train_label)\n",
    "#Shuffling\n",
    "order=shuffle(range(numbertr),random_state=100)\n",
    "train_label=train_label[order]\n",
    "train_data=train_data[order,0::]\n",
    "train_label = train_label.reshape( (numbertr, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting between training set and cross-validation set. In this exercise, 90% of data will be used for training and 10% for test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnb=0.9 # Fraction used for training\n",
    "valid_data=train_data[int(trainnb*numbertr):numbertr,0::]\n",
    "valid_label=train_label[int(trainnb*numbertr):numbertr]\n",
    "\n",
    "train_data=train_data[0:int(trainnb*numbertr),0::]\n",
    "train_label=train_label[0:int(trainnb*numbertr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the input variables. Here in this exercise, we have only two variables: `dR` and `mbb`. This one you can add more variables by modifying the [ana.C](https://github.com/monttj/CMSDAS2019/blob/master/ana.C) macro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvar = train_data.shape[1]\n",
    "print(\"number of input variables = \" , nvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 50 # number of nodes\n",
    "b = 0.08 # dropout probability\n",
    "#init\n",
    "init = 'glorot_uniform'#called \"Xavier uniform initializer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build model now and save it to `model.h5`. Adam optimizer is used in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(nvar,))\n",
    "x = Dense(a, activation='relu', kernel_initializer=init, bias_initializer='zeros')(inputs)\n",
    "x = Dropout(b)(x)\n",
    "x = Dense(a, activation='relu', kernel_initializer=init, bias_initializer='zeros')(x)\n",
    "x = Dropout(b)(x)\n",
    "x = Dense(a, activation='relu', kernel_initializer=init, bias_initializer='zeros')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam=keras.optimizers.Adam(lr=1E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1E-3)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','binary_accuracy'])\n",
    "\n",
    "modelfile = 'model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training with 20 epochs with batch size of 1024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(modelfile, monitor='val_binary_accuracy', verbose=1, save_best_only=False)\n",
    "\n",
    "history = model.fit(train_data, train_label,\n",
    "                             epochs=20, batch_size=1024,\n",
    "                             validation_data=(valid_data,valid_label),\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will check the performance by looking at the loss as a function of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Binary crossentropy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'],loc='upper right')\n",
    "plt.savefig(os.path.join(\"./\",'fig_score_loss.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the accuracy as a function of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'], loc='lower right')\n",
    "plt.savefig(os.path.join(\"./\",'fig_score_acc.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Can you lower the loss and increase the accuracy by changing the number of layers or optimize the parameters or adding more input features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
